{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 00 \u2013 Metric Smoke Test\n\n",
        "This notebook is a quick **sanity check** for the local metric implementation\n",
        "in `src/santa2025/evaluation.py`.\n\n",
        "It will:\n",
        "1. Import your metric functions from `santa2025.evaluation`.\n",
        "2. Run them on `data/raw/sample_submission.csv` (or any other submission).\n",
        "3. Show the per\u2011`n` table and total score.\n",
        "4. Let you compare the **local total score** against the **Kaggle leaderboard**\n",
        "   for the same CSV.\n\n",
        "If the local score matches the Kaggle score (up to tiny floating\u2011point noise),\n",
        "then your metric is wired up correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n\n",
        "pd.set_option('display.float_format', '{:.12f}'.format)\n\n",
        "PROJECT_ROOT = Path('.').resolve()\n",
        "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
        "print('Project root:', PROJECT_ROOT)\n",
        "print('Raw data dir:', DATA_RAW)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# This assumes you have `src/` marked as a Sources Root in PyCharm\n",
        "# and that `src/santa2025/evaluation.py` defines\n",
        "#   - metric_from_df(df: pd.DataFrame) -> pd.DataFrame\n",
        "#   - evaluate_submission_csv(path: str | Path) -> (score_table: pd.DataFrame, total_score: float)\n\n",
        "from santa2025.evaluation import metric_from_df, evaluate_submission_csv\n",
        "metric_from_df, evaluate_submission_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "sample_path = DATA_RAW / 'sample_submission.csv'\n",
        "if not sample_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f'Missing {sample_path}. Put Kaggle\\'s sample_submission.csv into data/raw/.'\n",
        "    )\n\n",
        "score_table, local_total = evaluate_submission_csv(sample_path)\n",
        "display(score_table.head())\n",
        "print('\\nLocal total score:', f'{local_total:.12f}')\n",
        "print('Distinct n:', score_table['n'].nunique())\n",
        "print('Min n:', score_table['n'].min(), 'Max n:', score_table['n'].max())\n",
        "print('Total trees:', score_table['count'].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare with Kaggle leaderboard\n\n",
        "To verify that your local metric is correct:\n\n",
        "1. Take the **same exact CSV** you just evaluated locally (e.g. `sample_submission.csv`\n",
        "   or a new submission you created with your packer).\n",
        "2. Upload it to the Kaggle **Santa 2025 \u2013 Christmas Tree Packing** competition.\n",
        "3. Wait for the submission to score, then copy the **score shown on the leaderboard**.\n",
        "4. Paste that score into the next cell as `kaggle_total`.\n",
        "5. Run the cell to see the absolute difference.\n\n",
        "Ideally, the difference should be **0** or on the order of numerical noise\n",
        "(e.g. < 1e-9). If there is a large difference, there is a bug in the local\n",
        "metric implementation that needs to be fixed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Paste the Kaggle leaderboard score for the SAME CSV here.\n",
        "# Example: kaggle_total = 123.456789012345\n",
        "kaggle_total = None  # <-- replace None with the numeric score from Kaggle\n\n",
        "if kaggle_total is None:\n",
        "    print('Set kaggle_total to the numeric score from Kaggle and re-run this cell.')\n",
        "else:\n",
        "    diff = abs(local_total - kaggle_total)\n",
        "    print('Local total:', f'{local_total:.12f}')\n",
        "    print('Kaggle total:', f'{kaggle_total:.12f}')\n",
        "    print('Absolute difference:', f'{diff:.12e}')\n",
        "    if diff < 1e-9:\n",
        "        print('\u2705 Metrics match within tight numerical tolerance.')\n",
        "    else:\n",
        "        print('\u26a0\ufe0f Difference is larger than expected. Double-check evaluation.py.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}